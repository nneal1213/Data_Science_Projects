{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import time\n",
    "from fake_useragent import UserAgent\n",
    "import random \n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import math\n",
    "import json,pymongo\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.utils.extmath import randomized_svd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from gensim.summarization import summarize\n",
    "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
    "import gensim\n",
    "\n",
    "from nltk.util import ngrams\n",
    "from nltk.tokenize import word_tokenize, wordpunct_tokenize, WhitespaceTokenizer\n",
    "import PyPDF2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scraping job urls from Glassdoor.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31\r"
     ]
    }
   ],
   "source": [
    "def parse_job_url(url):\n",
    "\n",
    "    try:    \n",
    "        ua = UserAgent()\n",
    "        user_agent = {'User-agent':ua.random}   \n",
    "        response = requests.get(url,headers = user_agent,timeout=10)\n",
    "    except Exception as e:\n",
    "        print(e.message)\n",
    "\n",
    "    page = response.text\n",
    "    soup = BeautifulSoup(page,\"lxml\")\n",
    "\n",
    "    job_url_list = list()\n",
    "    soup_object = soup.find_all('li',class_ = 'jl')\n",
    "\n",
    "    for i in soup_object:\n",
    "        job_url_list.append(i.div.a['href'])\n",
    "    \n",
    "    return job_url_list\n",
    "\n",
    "# Getting data scientist job urls within San Francisco\n",
    "\n",
    "main_url_list = [\n",
    "'https://www.glassdoor.com/Job/san-jose-data-scientist-jobs-SRCH_IL.0,8_IC1147436_KO9,23_IP',\n",
    "'https://www.glassdoor.com/Job/los-angeles-data-scientist-jobs-SRCH_IL.0,11_IC1146821_KO12,26_IP',\n",
    "'https://www.glassdoor.com/Job/boston-data-scientist-jobs-SRCH_IL.0,6_IC1154532_KO7,21_IP'\n",
    "    \n",
    "]\n",
    "\n",
    "job_url_list = list()\n",
    "\n",
    "for main_url in main_url_list:\n",
    "    \n",
    "    for n in range(1,32):\n",
    "        print(n,end='\\r')\n",
    "        url = main_url+str(n)+'.htm'\n",
    "        job_url_list.extend(parse_job_url(url))\n",
    "        time.sleep(6*random.random()+2)\n",
    "        if len(job_url_list) == 0: \n",
    "            print('Did not extract url')\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving webscraped information to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open ('/Users/nealcheng/Desktop/data/data_scientist_urls2', 'wb') as f:\n",
    "    pickle.dump(list(set(job_url_list)), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open ('/Users/nealcheng/Desktop/data/data_scientist_urls', 'rb') as f:\n",
    "    new_job_url_list = [('http://www.glassdoor.com'+ x) for x in pickle.load(f)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text_list = [np.nan for x in range(len(new_job_url_list))]\n",
    "company_rating_list = [np.nan for x in range(len(new_job_url_list))]\n",
    "job_title_list = [np.nan for x in range(len(new_job_url_list))]\n",
    "city_list = [np.nan for x in range(len(new_job_url_list))]\n",
    "company_list = [np.nan for x in range(len(new_job_url_list))]\n",
    "employer_id_list = [np.nan for x in range(len(new_job_url_list))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scraping job information from Glassdoor.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "failed_extraction_list = list()\n",
    "\n",
    "for n in range(len(new_job_url_list)):    \n",
    "\n",
    "    url = new_job_url_list[n]\n",
    "    print(n,end='\\r')\n",
    "    try:    \n",
    "        ua = UserAgent()\n",
    "        user_agent = {'User-agent':ua.random}   \n",
    "        response = requests.get(url,headers = user_agent,timeout=10)\n",
    "        time.sleep(3*random.random()+5)\n",
    "        \n",
    "    except: # Exception as e:\n",
    "        #print(e.message)\n",
    "        failed_extraction_list.append(n)\n",
    "        time.sleep(20*random.random())\n",
    "        #break\n",
    "        \n",
    "    page = response.text\n",
    "    soup = BeautifulSoup(page,\"lxml\")\n",
    "\n",
    "    try: \n",
    "        text_body = soup.find('div',id = 'JobDescContainer').div.text\n",
    "    except:\n",
    "        text_body = np.nan\n",
    "        failed_extraction_list.append(n)\n",
    "    \n",
    "    try:\n",
    "        company_rating = float(soup.find('span',class_ = 'compactStars margRtSm').text.strip())\n",
    "    except:\n",
    "        company_rating = np.nan\n",
    "        \n",
    "    try:\n",
    "        job_title = soup.find('h2').text    \n",
    "    except:\n",
    "        job_title = np.nan\n",
    "    \n",
    "    try: \n",
    "        city = soup.find('span',class_='subtle ib').text\n",
    "    except:\n",
    "        city = np.nan\n",
    "        \n",
    "    try:\n",
    "        company = soup.find('span',class_='ib padRtSm').text.strip()\n",
    "    except:\n",
    "        company = np.nan\n",
    "    \n",
    "    try:    \n",
    "        employer_id = soup.find('div',id = 'EmpBasicInfo')['data-emp-id']\n",
    "    except:\n",
    "        employer_id = np.nan\n",
    "        \n",
    "    text_list[n] = text_body\n",
    "    company_rating_list[n] = company_rating\n",
    "    job_title_list[n] = job_title\n",
    "    city_list[n] = city\n",
    "    company_list[n] = company\n",
    "    employer_id_list[n] = employer_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data = {'Company':company_list,'EmployerID':employer_id_list,\n",
    "                    'JobTitle':job_title_list,'City':city_list,\n",
    "                     'CompanyRating':company_rating_list,'Url':new_job_url_list,\n",
    "                    'Text':text_list                    \n",
    "                    })\n",
    "df['City'] = df['City'].str.replace('–','').str.strip()\n",
    "df.drop_duplicates(inplace=True)\n",
    "df.dropna(subset=['Text'],inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "with open ('/Users/nealcheng/Desktop/data/data_scientist_jobs', 'wb') as f:\n",
    "    pickle.dump(df, f)\n",
    "    \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Opening dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open ('/Users/nealcheng/Desktop/data/data_scientist_jobs', 'rb') as f:\n",
    "    df2 = pd.read_pickle(f)\n",
    "    df2.drop_duplicates(subset= ['Text'],inplace=True)\n",
    "    df2.drop_duplicates(subset= ['Url'],inplace=True)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating Tfidf Vectors for the job postings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(stop_words='english',sublinear_tf=True)\n",
    "tfidf_job_desc_data = tfidf.fit_transform(df2['Text']).todense()\n",
    "tfidf_job_desc_data -= np.mean(tfidf_job_desc_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating pca object with 500 components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pca = PCA(n_components=500)\n",
    "tfidf_job_desc_pca = pca.fit_transform(tfidf_job_desc_data)\n",
    "np.cumsum(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Writing a function to open a sample resume I had scraped from Indeed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Using sample Resume\n",
    "def open_resume(file):\n",
    "\n",
    "    pdf_file = open(file, 'rb')\n",
    "    read_pdf = PyPDF2.PdfFileReader(pdf_file)\n",
    "    number_of_pages = read_pdf.getNumPages()\n",
    "    page_content = str()\n",
    "    for n in range(number_of_pages):\n",
    "        page = read_pdf.getPage(n)\n",
    "        page_content += page.extractText()\n",
    "    return page_content\n",
    "\n",
    "page_content = open_resume('/Users/nealcheng/Desktop/data/Young-Lee.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the TFIDF vectors for the resume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf_resume_data = tfidf.transform([page_content])\n",
    "tfidf_resume_data = tfidf_resume_data.todense()-np.mean(tfidf_resume_data)\n",
    "norm_dist_resume_pca = pca.transform(tfidf_resume_data)\n",
    "\n",
    "cosine_list = list()\n",
    "\n",
    "for text in df2['Text']:\n",
    "    \n",
    "    tfidf = TfidfVectorizer(stop_words='english',sublinear_tf=True)\n",
    "    temp_tfidf_data = tfidf.transform([text])\n",
    "    temp_tfidf_data = temp_tfidf_data.todense() - np.mean(temp_tfidf_data)\n",
    "    \n",
    "    temp_pca_data = pca.transform(temp_tfidf_data)\n",
    "    cosine_list.append(np.sum(cosine_similarity(temp_pca_data,norm_dist_resume_pca)))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Cosine</th>\n",
       "      <th>Text</th>\n",
       "      <th>Url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>227</td>\n",
       "      <td>0.222575</td>\n",
       "      <td>At eHealth, we are passionate about solving ou...</td>\n",
       "      <td>http://www.glassdoor.com/partner/jobListing.ht...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>818</td>\n",
       "      <td>0.184928</td>\n",
       "      <td>About Optimizely: Our mission is to turn data ...</td>\n",
       "      <td>http://www.glassdoor.com/partner/jobListing.ht...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>114</td>\n",
       "      <td>0.181740</td>\n",
       "      <td>Data Scientist  Tasks include Developing “cor...</td>\n",
       "      <td>http://www.glassdoor.com/partner/jobListing.ht...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>655</td>\n",
       "      <td>0.181644</td>\n",
       "      <td>Why is This a Great Opportunity?  Our client ...</td>\n",
       "      <td>http://www.glassdoor.com/partner/jobListing.ht...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>827</td>\n",
       "      <td>0.171596</td>\n",
       "      <td>Company DescriptionClient of Roljobs Technolog...</td>\n",
       "      <td>http://www.glassdoor.com/partner/jobListing.ht...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index    Cosine                                               Text  \\\n",
       "0    227  0.222575  At eHealth, we are passionate about solving ou...   \n",
       "1    818  0.184928  About Optimizely: Our mission is to turn data ...   \n",
       "2    114  0.181740   Data Scientist  Tasks include Developing “cor...   \n",
       "3    655  0.181644   Why is This a Great Opportunity?  Our client ...   \n",
       "4    827  0.171596  Company DescriptionClient of Roljobs Technolog...   \n",
       "\n",
       "                                                 Url  \n",
       "0  http://www.glassdoor.com/partner/jobListing.ht...  \n",
       "1  http://www.glassdoor.com/partner/jobListing.ht...  \n",
       "2  http://www.glassdoor.com/partner/jobListing.ht...  \n",
       "3  http://www.glassdoor.com/partner/jobListing.ht...  \n",
       "4  http://www.glassdoor.com/partner/jobListing.ht...  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3 = pd.DataFrame({'Cosine':cosine_list,'Url':df2['Url'],'Text':df2['Text']}).sort_values('Cosine',ascending=False).reset_index()\n",
    "df3.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best matching job position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" Business Intelligence Senior Analyst (TEMPORARY) ESSENTIAL JOB FUNCTIONS:Using Qlikview to respond to BI requests from throughout the business by delivering reports and visualizations.Create, modify, and enhance Qlikview dashboardsDevelop automated reports which combine data from disparate sources to facilitate day-to-day business operationsSetup data Extract-Transform-Load (ETL) processes within the Business Intelligence tool to combine various data sourcesPresent, educate, and champion the use of business intelligence tools to gain business insights across all levels of the organization.Assist with miscellaneous reportsMINIMUM QUALIFICATIONS:Expertise using Qlikview, Oracle BI, and SQL Plus.4+ years of experience in a Business Intelligence Analyst role or equivalent talent with dataExpertise with Excel and SQL, to include developing and maintaining advanced queries, pivot tables, and data modelsExpertise developing advanced data extracts and data transformationsMust possess strong analytical and problem solving skills, with strict attention to accuracy and detailMust possess strong oral and written communication skillsMust be an energetic individual, able to work independently, able to perform to high standards of productivity, able to multi-task, and able and willing to meet critical deadlinesBachelors degree of relevance or equivalent experienceDESIRED QUALIFICATIONS:It's an added benefit to our company if you also possess additional BI skills such as with MySQL and/or open source geospatial platforms.LOCATION:This job can be performed out of our offices in San Francisco, CA or Gold River, CA. Posted by StartWire\""
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3['Text'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resume text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Young Lee\\nData Scientist - Allstate\\nDublin, CA\\n-\\nEmail me on Indeed: \\nindeed.com/r/Young-Lee/109dcde6f5b08add\\nData scientist with experience in property & casualty personal lines insurance and academic background in\\nstatistics and economics.\\nModeling\\n-- Regularized GLMs (LASSO, Ridge, Elastic net), Gradient Boosting Machines, Generalized Additive Models\\n-- Insurance loss modeling using Tweedie regressions\\n-- Longitudinal survey data analysis\\nComputing/Programming\\n-- R, SQL, Linux, Python, Stata\\nWilling to relocate to: San Francisco, CA - San Jose, CA - Menlo Park, CA\\nAuthorized to work in the US for any employer\\nWORK EXPERIENCE\\nData Scientist\\nAllstate\\n \\n-\\n \\nChicago, IL\\n-\\nAugust 2015 to Present\\nBuilt and deployed XGBoost model to predict likelihood of a policy having an undisclosed driver,\\npotentially saving company in excess of $3,200,000/year\\nŁ Performed predictive modeling of auto insurance losses and life insurance mortality using elastic net\\nregularized generalized linear models, generalized additive models, and gradient boosting machines\\nwritten in R within a Linux server environment\\nŁ Managed project to refit insurance pricing plan, working closely with actuaries to incorporate necessary\\nadjustments for variations in state regulations from data preparation through model building and\\nimplementation\\nŁ Automated creation of visualizations that compare different optimization scenarios\\nŁ Built program for a non-technical audience that automatically scores models, bringing data together\\nfrom multiple data sources\\nVolunteer GED Tutor\\nENoK\\n \\n-\\n \\nChicago, IL\\n-\\nJanuary 2015 to September 2016\\nTutored GED English and math to North Korean refugee students, covering English writing, reading\\ncomprehension, and algebra\\nEDUCATION\\nMaster of Science in Statistics\\nUniversity of Chicago\\n \\n-\\n \\nChicago, IL\\nJune 2016\\nMaster's in Paper\\nUniversity of California\\n \\n-\\n \\nLos Angeles, CA\\nSKILLS\\nHadoop (2 years)\\n, \\nHive (2 years)\\n, \\nLinux (2 years)\\n, \\nSQL (2 years)\\n, \\nR (6 years)\\n, \\nPython (2 years)\\nLINKS\\nhttp://linkedin.com/in/yclee1\\nADDITIONAL INFORMATION\\nSKILLS & INTERESTS\\nProgramming: R, SQL (Oracle), Linux/Unix, Python, Hadoop/Hive, Spark\\nForeign Language: Bilingual Korean (TOPIK Level 5), Elementary Spanish\\n\""
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
